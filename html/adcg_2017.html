<!DOCTYPE html>
<html>
<head>
  <title>Annotated bibliography</title>
  <link href='../css/style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="../">Annotated bibliography</a>
  </div>
  <div id="container">
<h1 id="the-alternating-descent-conditional-gradient-method-for-sparse-inverse-problems"><a href="https://epubs.siam.org/doi/pdf/10.1137/15M1035793">The Alternating Descent Conditional Gradient method for sparse inverse problems</a></h1>
<p>Solving a sparse inverse problem amounts to finding a small set of parameters and weights that explain the data. When the parameter spaces are infinite (for e.g.&#160;when the parameters are the positions and velocities of sources), the optimisation problem must operate over an infinite-dimensional space.</p>
<p>In the literature, there are three proposals for getting around solving an infinite-dimensional optimisation problem.</p>
<ul>
<li><p>The first suggestion is to discretise on a grid. However, this is only feasible for small parameter spaces</p></li>
<li><p>The second is to employ spectral techniques or semidefinite programming. However, these methods are limited in applicability and have large computational times. They are also sensitive to noise and estimates of the number of sources (see )</p></li>
<li><p>The third proposal is to use atomic norms over data (see )</p></li>
</ul>
<p>In this paper, the authors augment the conditional gradient method (CGM), also known as the Frank-Wolfe method, with non-convex local searches that exploit the structure of the parameter space. This method, called the alternating descent conditional gradient (ADCG) method, combines the speed of non-convex approaches with the convergence guarantees of convex optimisation and is suitable for sparse inverse problems with <em>differentiable</em> observation models.</p>
<h2 id="mathematical-setup">Mathematical setup</h2>
<p>The goal of the paper is to recover the <em>number</em>, <em>weights</em> and <em>parameters</em> of a number of sources given a single noisy observation. Consider a set of sources with weights <span class="math inline">\(w\)</span> and parameters <span class="math inline">\(\theta\)</span>. The weight may encode the intensity of a source and the parameter the position.</p>
<p>The observation model is given by a function <span class="math inline">\(\psi: \Theta \rightarrow \mathbb{R}^d\)</span>. Accordingly, the noise-free observation generated by a weighted collection of sources (also called signal parameters) is given by <span class="math display">\[
\{(w_i, \theta_i)\}_{i=1}^{K} \mapsto \sum_{i=1}^{K} w_i \psi(\theta_i) \in \mathbb{R}^d
\]</span> The function <span class="math inline">\(\psi\)</span> may <em>not</em> be linear, but has to be <em>bounded</em> (<span class="math inline">\(||\psi(\theta)||^2_2 \leq 1\)</span>) for all <span class="math inline">\(\theta\)</span> and <em>differentiable</em> in <span class="math inline">\(\theta\)</span>.</p>
<p>The noisy observation is given by <span class="math display">\[
y = \sum_{i=1}^{\tilde{K}} \tilde{w}_i \psi(\tilde{\theta}_i) + \nu,
\]</span> where <span class="math inline">\(\nu\)</span> is the noise term. The goal is thus to recover an estimate of the true parameters <span class="math inline">\(\{(\tilde{w}_i, \tilde{\theta}_i)\}\)</span> given <span class="math inline">\(y\)</span>.</p>
<p>The authors encode the weighted collection of sources as an atomic measure <span class="math inline">\(\mu\)</span> on <span class="math inline">\(\Theta\)</span>, with mass <span class="math inline">\(w_i\)</span> at point <span class="math inline">\(\theta_i\)</span>, such that <span class="math inline">\(\mu = \sum_{i=1}^{K} w_i \delta_{\theta_{i}}\)</span>. The action of the forward operator <span class="math inline">\(\Phi\)</span> is given by <span class="math display">\[
\Phi \mu = \int \psi(\theta) d\mu(\theta).
\]</span> The problem is then to recover <span class="math inline">\(\mu_{\text{true}}\)</span> given <span class="math inline">\(y = \Phi \mu_{\text{true}} + \nu\)</span> and can be written down as an optimisation problem oover the Banach space of bounded, signed measres on the measurable space <span class="math inline">\(\Theta\)</span> equipped with the total variation norm. The optimisation problem is thus <span class="math display">\[
\begin{align}
\text{minimise}\quad &amp; l(\Phi\mu - y) \\
\text{subject to}\quad &amp; |\mu|(\Theta) \leq \tau,
\end{align}
\]</span> where <span class="math inline">\(l\)</span> is a differentiable, convex loss. This problem is a continuous analogue of the classic LASSO problem. In the standard LASSO, <span class="math inline">\(l(r) = ||r||_2^2\)</span> and <span class="math inline">\(\Theta = \{1,...,k\}\)</span>.</p>
<h2 id="review-of-cgm">Review of CGM</h2>
<p>The traditional CGM solves an optimisation problem of the type <span class="math display">\[
\begin{align}
\text{minimise}_{x \in \mathcal{C}} \quad &amp; f(x),
\end{align}
\]</span> where <span class="math inline">\(\mathcal{C}\)</span> is some convex set and <span class="math inline">\(f\)</span> is a differentiable convex function.</p>
<p>The CGM solves linearised versions of the equation above at each iteration. The linear approximation to <span class="math inline">\(f\)</span> at <span class="math inline">\(x_k\)</span> (at iteration <span class="math inline">\(k\)</span>) is given by <span class="math display">\[
\hat{f}_k(s) = f(x_k) + f^\prime(s-x_k;x_k).
\]</span> Here, <span class="math inline">\(f^\prime(s-x_k;x_k)\)</span> is the directional derivative of <span class="math inline">\(f(x_k)\)</span> along <span class="math inline">\((s-x_k)\)</span>. As <span class="math inline">\(f\)</span> is convex, <span class="math inline">\(\hat{f}_k(s)\)</span> is a global lower bound. In CGM, the linear approximation is minimised to obtain a solution, <span class="math inline">\(s_k\)</span>, at iteration <span class="math inline">\(k\)</span>. This solution mnimises the linear approximation which itself decays with distance from <span class="math inline">\(x_k\)</span>. Therefore, a convex combination of <span class="math inline">\(s_k\)</span> and <span class="math inline">\(x_k\)</span> is taken as the next iterate.</p>
<h2 id="cgm-for-sparse-inverse-problems">CGM for sparse inverse problems</h2>
<p>When applying CGM to the space of measures, each iterate is a sparse measure <span class="math inline">\(\mu_k\)</span> supported on <span class="math inline">\(N_k\)</span> points <span class="math display">\[
\mu_k = \sum_{i=1}^{N_k} w_i^{(k)} \delta_{\theta_i^{(k)}}.
\]</span> The measures <span class="math inline">\(\mu_k\)</span> are represented on a computer by the pair of vectors <span class="math inline">\(w_k \in \mathbb{R}^{N_k}\)</span> and <span class="math inline">\(\vec{\theta}_k \in \Theta^{N_k}\)</span></p>
<p>The function <span class="math inline">\(f(\mu)\)</span> to be mimimised is <span class="math inline">\(l(\Phi\mu - y)\)</span>. The linearised objective to be minimised is <span class="math inline">\(\hat{f}_k(s) = f(\mu_k) + f^\prime(s;\mu_k)\)</span>. The directional derivative is shown to be given by <span class="math inline">\(f^\prime(s;\mu_k) = l^\prime(\Phi s; r_k) = &lt;\nabla l(r_k), \Phi s&gt;\)</span>, where <span class="math inline">\(r_k = \Phi \mu_k - y\)</span> is the residual at iteration <span class="math inline">\(k\)</span>. The optimisation problem that has to be solved is <span class="math display">\[
\begin{align}
\text{minimise}_{|s|(\Theta) \leq \tau} \quad &amp; \int F(\theta) ds(\theta),
\end{align}
\]</span> where <span class="math inline">\(F(\theta) = &lt;\nabla l(r_k), \psi(\theta)&gt;\)</span>.</p>
<p>Effectively, the CGM-M (conditional gradient method for measures) algorithm alternates between selecting a source to add to the support and tuning the weights to lower the current cost. In practice, the method does not do very well. One of the reasons for this is that the algorithm can only optimise by adding or removing sources and not by moving the set of sources smoothly within the parameter space <span class="math inline">\(\Theta.\)</span></p>
<h2 id="adcg">ADCG</h2>
<p>Convergence can be sped up and sparser solutions than those found by CGM-M can be found by allowing the support to move continuously within <span class="math inline">\(\Theta\)</span>. ADCG does this by locally improving the support at each iteration. To do this, after performing descent on the convex optimisation problem given by the linearised approximation (i.e.&#160;adding a new source to the support), the algorithm does an additional descent on the non-convex problem shown below <span class="math display">\[
\begin{align}
\text{minimise} \quad &amp; l\Big(\sum_{i=1}^{m} w_i \phi(\theta_i) - y\Big) \\
\text{subject to} \quad &amp; \vec{\theta} \in \Theta^m, ||w||_1 \leq \tau.
\end{align}
\]</span> This is done by block coordinate descent on <span class="math inline">\(w\)</span> and <span class="math inline">\(\vec{\theta}\)</span>.</p>
<h2 id="numerical-results">Numerical results</h2>
<p>The paper shows that ADCG achieves state-of-the-art results for several problems, including localisation of sources in fluorescence microscopy.</p>
<p>To use ADCG, the user has to provide two additional routines: one that evaluates the observation model and its derivatives and another that approximately solves the linear minimisation problem of updating sources. A Julia implementation of ADCG can be found <a href="https://github.com/nboyd/SparseInverseProblems.jl">here.</a></p>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </div>

  <script type="text/javascript" src="../js/mathjax_config.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-90310997-2', 'auto');
		ga('send', 'pageview');
	</script>
</body>
</html>
