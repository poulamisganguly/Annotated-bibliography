# [The Alternating Descent Conditional Gradient method for sparse inverse problems](https://epubs.siam.org/doi/pdf/10.1137/15M1035793) 

Solving a sparse inverse problem amounts to finding a small set of parameters and weights that explain the data. When the parameter spaces are infinite (for e.g. when the parameters are the positions and velocities of sources), the optimisation problem must operate over an infinite-dimensional space. 


In the literature, there are three proposals for getting around solving an infinite-dimensional optimisation problem.

- The first suggestion is to discretise on a grid. However, this is only feasible for small parameter spaces

- The second is to employ spectral techniques or semidefinite programming. However, these methods are limited in applicability and have large computational times. They are also sensitive to noise and estimates of the number of sources (see )

- The third proposal is to use atomic norms over data (see )


In this paper, the authors augment the conditional gradient method (CGM), also known as the Frank-Wolfe method, with non-convex local searches that exploit the structure of the parameter space. This method, called the alternating descent conditional gradient (ADCG) method, combines the speed of non-convex approaches with the convergence guarantees of convex optimisation and is suitable for sparse inverse problems with _differentiable_ observation models.

## Mathematical setup

The goal of the paper is to recover the *number*, *weights* and *parameters* of a number of sources given a single noisy observation. Consider a set of sources with weights $w$ and parameters $\theta$. The weight may encode the intensity of a source and the parameter the position. 


The observation model is given by a function $\psi: \Theta \rightarrow \mathbb{R}^d$. Accordingly, the noise-free observation generated by a weighted collection of sources (also called signal parameters) is given by
$$
\{(w_i, \theta_i)\}_{i=1}^{K} \mapsto \sum_{i=1}^{K} w_i \psi(\theta_i) \in \mathbb{R}^d
$$
The function $\psi$ may *not* be linear, but has to be *bounded* ($||\psi(\theta)||^2_2 \leq 1$) for all $\theta$ and *differentiable* in $\theta$.


The noisy observation is given by
$$
y = \sum_{i=1}^{\tilde{K}} \tilde{w}_i \psi(\tilde{\theta}_i) + \nu,
$$
where $\nu$ is the noise term. The goal is thus to recover an estimate of the true parameters $\{(\tilde{w}_i, \tilde{\theta}_i)\}$ given $y$.


The authors encode the weighted collection of sources as an atomic measure $\mu$ on $\Theta$, with mass $w_i$ at point $\theta_i$, such that $\mu = \sum_{i=1}^{K} w_i \delta_{\theta_{i}}$. The action of the forward operator $\Phi$ is given by
$$
\Phi \mu = \int \psi(\theta) d\mu(\theta).
$$
The problem is then to recover $\mu_{\text{true}}$ given $y = \Phi \mu_{\text{true}} + \nu$ and can be written down as an optimisation problem oover the Banach space of bounded, signed measres on the measurable space $\Theta$ equipped with the total variation norm. The optimisation problem is thus
$$
\begin{align}
\text{minimise}\quad & l(\Phi\mu - y) \\
\text{subject to}\quad & |\mu|(\Theta) \leq \tau,
\end{align}
$$
where $l$ is a differentiable, convex loss. This problem is a continuous analogue of the classic LASSO problem. In the standard LASSO, $l(r) = ||r||_2^2$ and $\Theta = \{1,...,k\}$.

## Review of CGM

The traditional CGM solves an optimisation problem of the type
$$
\begin{align}
\text{minimise}_{x \in \mathcal{C}} \quad & f(x),
\end{align}
$$
where $\mathcal{C}$ is some convex set and $f$ is a differentiable convex function.


The CGM solves linearised versions of the equation above at each iteration. The linear approximation to $f$ at $x_k$ (at iteration $k$) is given by
$$
\hat{f}_k(s) = f(x_k) + f^\prime(s-x_k;x_k).
$$
Here, $f^\prime(s-x_k;x_k)$ is the directional derivative of $f(x_k)$ along $(s-x_k)$. As $f$ is convex, $\hat{f}_k(s)$ is a global lower bound. In CGM, the linear approximation is minimised to obtain a solution, $s_k$, at iteration $k$. This solution mnimises the linear approximation which itself decays with distance from $x_k$. Therefore, a convex combination of $s_k$ and $x_k$ is taken as the next iterate.

## CGM for sparse inverse problems

When applying CGM to the space of measures, each iterate is a sparse measure $\mu_k$ supported on $N_k$ points
$$
\mu_k = \sum_{i=1}^{N_k} w_i^{(k)} \delta_{\theta_i^{(k)}}.
$$
The measures $\mu_k$ are represented on a computer by the pair of vectors $w_k \in \mathbb{R}^{N_k}$ and $\vec{\theta}_k \in \Theta^{N_k}$


The function $f(\mu)$ to be mimimised is $l(\Phi\mu - y)$. The linearised objective to be minimised is $\hat{f}_k(s) = f(\mu_k) + f^\prime(s;\mu_k)$. The directional derivative is shown to be given by $f^\prime(s;\mu_k) = l^\prime(\Phi s; r_k) = <\nabla l(r_k), \Phi s>$, where $r_k = \Phi \mu_k - y$ is the residual at iteration $k$. The optimisation problem that has to be solved is
$$
\begin{align}
\text{minimise}_{|s|(\Theta) \leq \tau} \quad & \int F(\theta) ds(\theta),
\end{align}
$$
where $F(\theta) = <\nabla l(r_k), \psi(\theta)>$.

Effectively, the CGM-M (conditional gradient method for measures) algorithm alternates between selecting a source to add to the support and tuning the weights to lower the current cost. In practice, the method does not do very well. One of the reasons for this is that the algorithm can only optimise by adding or removing sources and not by moving the set of sources smoothly within the parameter space $\Theta.$

## ADCG

Convergence can be sped up and sparser solutions than those found by CGM-M can be found by allowing the support to move continuously within $\Theta$. ADCG does this by locally improving the support at each iteration. To do this, after performing descent on the convex optimisation problem given by the linearised approximation (i.e. adding a new source to the support), the algorithm does an additional descent on the non-convex problem shown below
$$
\begin{align}
\text{minimise} \quad & l\Big(\sum_{i=1}^{m} w_i \phi(\theta_i) - y\Big) \\
\text{subject to} \quad & \vec{\theta} \in \Theta^m, ||w||_1 \leq \tau.
\end{align}
$$
This is done by block coordinate descent on $w$ and $\vec{\theta}$.

## Numerical results

The paper shows that ADCG achieves state-of-the-art results for several problems, including localisation of sources in fluorescence microscopy. 

To use ADCG, the user has to provide two additional routines: one that evaluates the observation model and its derivatives and another that approximately solves the linear minimisation problem of updating sources. A Julia implementation of ADCG can be found [here.](https://github.com/nboyd/SparseInverseProblems.jl)

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
