<!DOCTYPE html>
<html>
<head>
  <title>Annotated bibliography</title>
  <link href='../css/style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="../">Annotated bibliography</a>
  </div>
  <div id="container">
<h1 id="revisiting-frank-wolfe-projection-free-sparse-convex-optimization"><a href="http://proceedings.mlr.press/v28/jaggi13-supp.pdf">Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization</a></h1>
<p>This is a modern treatment of the conditional gradient or the Frank-Wolfe method, which was originally described in 1956. General constrained convex optimisation problems of the form <span class="math display">\[
\text{min}_{x \in \mathcal{D}} f(x),
\]</span> are considered. Here, <span class="math inline">\(f\)</span> is convex and continuously differentiable, and the domain <span class="math inline">\(\mathcal{D}\)</span> is a compact convex subset of any vector space.</p>
<p>The conditional gradient method amounts to linearising the objective function at a position <span class="math inline">\(x\)</span> and moving towards a minimiser of the linear function (over the same domain <span class="math inline">\(\mathcal{D}\)</span>). At each step of the algorithm, at most one point is added to the solution, thus making it possible to obtain sparse solutions</p>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </div>

  <script type="text/javascript" src="../js/mathjax_config.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-90310997-2', 'auto');
		ga('send', 'pageview');
	</script>
</body>
</html>
